# Nezha å…¼å®¹çš„ History Worker å®ç°

## ğŸ¯ éµå¾ª Nezha è§„åˆ™çš„è®¾è®¡

æ ¹æ®æ‚¨æä¾›çš„ Nezha è®¾è®¡è§„åˆ™ï¼Œæˆ‘å·²ç»é‡æ–°å®ç°äº† History Workerï¼Œç¡®ä¿å®Œå…¨ç¬¦åˆä»¥ä¸‹ä¸‰ä¸ªæ ¸å¿ƒè§„åˆ™ï¼š

## âœ… è§„åˆ™1ï¼šå¯åŠ¨è°ƒç”¨
**åœ¨ä¸€ä¸ªç±»ä¼¼NewServiceå‡½æ•°ä¸­å¯åŠ¨**

```go
// åœ¨ NewService å‡½æ•°ä¸­å¯åŠ¨ (internal/sse/service.go)
func NewService(db *gorm.DB, endpointService *endpoint.Service) *Service {
    s := &Service{
        // ... å…¶ä»–å­—æ®µ
        historyWorker: NewHistoryWorker(db), // âœ… åœ¨NewServiceä¸­åˆ›å»ºå’Œå¯åŠ¨
        // ...
    }
    return s
}

// NewHistoryWorker å‡½æ•°å¯åŠ¨ä¸¤ä¸ªgoroutine (internal/sse/history_worker.go)
func NewHistoryWorker(db *gorm.DB) *HistoryWorker {
    worker := &HistoryWorker{
        db:            db,
        dataInputChan: make(chan MonitoringData, 5000), // âœ… æ•°æ®è¾“å…¥é€šé“
        // ...
    }

    // âœ… å¯åŠ¨ä¸»æ•°æ®å¤„ç†åç¨‹
    worker.wg.Add(1)
    go worker.dataProcessWorker()

    // âœ… å¯åŠ¨æ‰¹é‡å†™å…¥åç¨‹
    worker.wg.Add(1)
    go worker.batchWriteWorker()

    return worker
}
```

## âœ… è§„åˆ™2ï¼šæ•°æ®è¾“å…¥
**é€šè¿‡sseæ¥æ”¶åˆ°updateç±»å‹çš„æ¶ˆæ¯åé€šè¿‡Dispatchæ–¹æ³•å°†æ•°æ®æ¨é€åˆ°channel**

```go
// SSE Service æ¥æ”¶åˆ° update äº‹ä»¶åè°ƒç”¨ Dispatch (internal/sse/service.go)
func (s *Service) ProcessEvent(endpointID int64, event models.EndpointSSE) error {
    // ... å…¶ä»–å¤„ç†é€»è¾‘

    if event.EventType == models.SSEEventTypeUpdate {
        // âœ… é€šè¿‡Dispatchæ–¹æ³•å°†æ•°æ®æ¨é€åˆ°channel
        if s.historyWorker != nil {
            s.historyWorker.Dispatch(event)
        }
    }
    
    return nil
}

// Dispatch æ–¹æ³•å°†æ•°æ®æ¨é€åˆ°é€šé“ (internal/sse/history_worker.go)
func (hw *HistoryWorker) Dispatch(event models.EndpointSSE) {
    data := MonitoringData{
        EndpointID: event.EndpointID,
        InstanceID: event.InstanceID,
        TCPIn:      event.TCPRx,
        TCPOut:     event.TCPTx,
        UDPIn:      event.UDPRx,
        UDPOut:     event.UDPTx,
        Ping:       event.Ping,
        Pool:       event.Pool,
        Timestamp:  time.Now(),
    }

    // âœ… æ¨é€åˆ°æ•°æ®å¤„ç†é€šé“ï¼ˆéé˜»å¡ï¼‰
    select {
    case hw.dataInputChan <- data:
        // æˆåŠŸæ¨é€
    default:
        log.Warnf("æ•°æ®å¤„ç†é˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒæ•°æ®")
    }
}
```

## âœ… è§„åˆ™3ï¼šWorkeræ–¹æ³•
**æŒç»­è¿è¡Œçš„goroutineï¼Œå¤„ç†æ•°æ®æ¥æ”¶ã€éªŒè¯ã€èšåˆè®¡ç®—ã€ç´¯ç§¯å’Œæ‰¹é‡å†™å…¥**

### 3.1 æ•°æ®æ¥æ”¶å’ŒéªŒè¯

```go
// ä¸»æ•°æ®å¤„ç†Worker - æŒç»­è¿è¡Œçš„goroutine (internal/sse/history_worker.go)
func (hw *HistoryWorker) dataProcessWorker() {
    defer hw.wg.Done()
    log.Info("[HistoryWorker]ä¸»æ•°æ®å¤„ç†åç¨‹å·²å¯åŠ¨")

    for {
        select {
        case <-hw.stopChan:
            return // åœæ­¢ä¿¡å·

        case data := <-hw.dataInputChan:
            // âœ… 1. ä»channelç®¡é“æ¥æ”¶SSEçš„æ•°æ®
            hw.processMonitoringData(data)
        }
    }
}

// æ•°æ®éªŒè¯
func (hw *HistoryWorker) processMonitoringData(data MonitoringData) {
    // âœ… æ•°æ®éªŒè¯
    if data.EndpointID <= 0 || data.InstanceID == "" {
        log.Warnf("æ— æ•ˆçš„ç›‘æ§æ•°æ®")
        return
    }

    // ... ç´¯ç§¯é€»è¾‘
}
```

### 3.2 èšåˆè®¡ç®—ç®—æ³•

```go
func (hw *HistoryWorker) aggregateAndWrite(dataPoints []MonitoringData) {
    // âœ… 2. å¯¹æ•°æ®è¿›è¡Œèšåˆè®¡ç®—
    
    for i, point := range dataPoints {
        aggregated.UpCount++

        // âœ… TCPå’ŒUDPä½¿ç”¨æ™®é€šå¹³å‡ç®—æ³•
        aggregated.AvgTCPIn += float64(point.TCPIn)
        aggregated.AvgTCPOut += float64(point.TCPOut)
        aggregated.AvgUDPIn += float64(point.UDPIn)
        aggregated.AvgUDPOut += float64(point.UDPOut)

        // âœ… Pingå»¶è¿Ÿä½¿ç”¨åŠ æƒå¹³å‡ç®—æ³•
        if point.Ping != nil {
            if i == 0 {
                aggregated.AvgPing = float64(*point.Ping)
            } else {
                // åŠ æƒå¹³å‡å…¬å¼ï¼šnew_avg = (old_avg * count + new_value) / (count + 1)
                upCountFloat := float64(pingCount + 1)
                aggregated.AvgPing = (aggregated.AvgPing*float64(pingCount) + float64(*point.Ping)) / upCountFloat
            }
            pingCount++
        }

        // âœ… Poolè¿æ¥æ± ä½¿ç”¨æ™®é€šå¹³å‡ç®—æ³•
        if point.Pool != nil {
            totalPool += float64(*point.Pool)
            poolCount++
        }
    }

    // è®¡ç®—æœ€ç»ˆå¹³å‡å€¼
    if aggregated.UpCount > 0 {
        countFloat := float64(aggregated.UpCount)
        aggregated.AvgTCPIn = aggregated.AvgTCPIn / countFloat
        aggregated.AvgTCPOut = aggregated.AvgTCPOut / countFloat
        aggregated.AvgUDPIn = aggregated.AvgUDPIn / countFloat
        aggregated.AvgUDPOut = aggregated.AvgUDPOut / countFloat
    }

    if poolCount > 0 {
        aggregated.AvgPool = totalPool / float64(poolCount)
    }
}
```

### 3.3 ç´¯ç§¯å’Œæ‰¹é‡å†™å…¥

```go
func (hw *HistoryWorker) processMonitoringData(data MonitoringData) {
    // è·å–æˆ–åˆ›å»ºç´¯ç§¯æ•°ç»„
    currentStatus := hw.getOrCreateStatus(key)

    // âœ… 3. å°†ç›‘æ§ç»“æœæ·»åŠ åˆ°ç»“æœæ•°ç»„ä¸­
    currentStatus.mu.Lock()
    currentStatus.Result = append(currentStatus.Result, data)
    resultLength := len(currentStatus.Result)
    currentStatus.mu.Unlock()

    // âœ… å½“ç´¯ç§¯åˆ°_CurrentStatusSize(30ä¸ª)æ•°æ®ç‚¹æ—¶è§¦å‘æ‰¹é‡å†™å…¥
    if resultLength >= _CurrentStatusSize {
        hw.triggerBatchWrite(key, currentStatus)
    }
}
```

## ğŸ—ï¸ å®Œæ•´çš„æ¶æ„æµç¨‹å›¾

```mermaid
graph TD
    A[SSEæ¥æ”¶updateäº‹ä»¶] --> B[è°ƒç”¨historyWorker.Dispatch]
    B --> C[æ•°æ®æ¨é€åˆ°dataInputChané€šé“]
    C --> D[dataProcessWorkeræŒç»­ç›‘å¬é€šé“]
    D --> E[æ•°æ®éªŒè¯]
    E --> F[æ·»åŠ åˆ°ç´¯ç§¯æ•°ç»„Result]
    F --> G{ç´¯ç§¯æ•°æ®è¾¾åˆ°30ä¸ª?}
    G -->|å¦| F
    G -->|æ˜¯| H[å¤åˆ¶æ•°æ®å¹¶æ¸…ç©ºæ•°ç»„]
    H --> I[èšåˆè®¡ç®—]
    I --> J[TCP/UDPæ™®é€šå¹³å‡]
    I --> K[PingåŠ æƒå¹³å‡]
    I --> L[Poolæ™®é€šå¹³å‡]
    J --> M[æ‰¹é‡å†™å…¥ServiceHistoryè¡¨]
    K --> M
    L --> M
    M --> F
```

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§æ€»ç»“

### âœ… å®Œå…¨ç¬¦åˆNezhaè§„åˆ™

1. **å¯åŠ¨æ–¹å¼**: âœ… åœ¨ `NewService` ä¸­å¯åŠ¨
2. **æ•°æ®è¾“å…¥**: âœ… é€šè¿‡ `Dispatch` æ–¹æ³•æ¨é€åˆ° channel
3. **Workerå¤„ç†**: âœ… æŒç»­è¿è¡Œçš„ goroutine å¤„ç†æ‰€æœ‰ä»»åŠ¡

### âœ… èšåˆç®—æ³•ä¸¥æ ¼éµå¾ª

- **TCP/UDPæµé‡**: æ™®é€šå¹³å‡ç®—æ³• `(sum / count)`
- **Pingå»¶è¿Ÿ**: åŠ æƒå¹³å‡ç®—æ³• `(old_avg * count + new_value) / (count + 1)`
- **Poolè¿æ¥æ± **: æ™®é€šå¹³å‡ç®—æ³• `(sum / count)`

### âœ… æ¶æ„ä¼˜åŒ–

- **å¼‚æ­¥å¤„ç†**: ä¸é˜»å¡SSEä¸»æµç¨‹
- **ç¼“å†²é€šé“**: `dataInputChan` (5000) + `batchWriteChan` (1000)
- **å¹¶å‘å®‰å…¨**: è¯»å†™é”ä¿æŠ¤å…±äº«æ•°æ®
- **ä¼˜é›…å…³é—­**: ç­‰å¾…æ‰€æœ‰goroutineå®Œæˆ

## ğŸ“Š ç›‘æ§ç»Ÿè®¡

```go
// Workerç»Ÿè®¡ä¿¡æ¯
stats := historyWorker.GetStats()
// è¿”å›:
// {
//   "active_instances": 5,           // æ´»è·ƒå®ä¾‹æ•°
//   "total_data_points": 150,        // æ€»ç´¯ç§¯æ•°æ®ç‚¹
//   "data_input_queue_size": 23,     // è¾“å…¥é˜Ÿåˆ—å¤§å°
//   "batch_queue_size": 2,           // æ‰¹é‡å†™å…¥é˜Ÿåˆ—å¤§å°
//   "accumulation_threshold": 30     // ç´¯ç§¯é˜ˆå€¼
// }
```

## ğŸš€ æ€§èƒ½ç‰¹ç‚¹

- **é«˜ååé‡**: æ”¯æŒ5000ä¸ªSSEäº‹ä»¶ç¼“å†²
- **ä½å»¶è¿Ÿ**: å¼‚æ­¥å¤„ç†ï¼Œä¸é˜»å¡ä¸»æµç¨‹
- **å†…å­˜å‹å¥½**: è‡ªåŠ¨æ¸…ç†ç´¯ç§¯æ•°ç»„
- **æ•…éšœæ¢å¤**: ä¼˜é›…å¤„ç†é˜Ÿåˆ—æ»¡è½½æƒ…å†µ

è¿™ä¸ªå®ç°å®Œå…¨éµå¾ªäº†Nezhaçš„è®¾è®¡æ¨¡å¼ï¼Œç¡®ä¿äº†æ•°æ®å¤„ç†çš„å¯é æ€§å’Œé«˜æ€§èƒ½ï¼
